裁判实验
对测试结果的开放式问题进行进一步评分。本项目使用自动化评分方法，旨在减少人工参与，提高评测体系的智能性。因此选择使用大模型组合判分的方式对开放式问题进行评分，此处进行裁判实验，

通过对比不同大模型组合在开放式问题上的表现，基于人工判分的基准，评估模型评分与人工评分的对齐程度，筛选出适合本次研究对开放式问题判分的最佳模型组合。以下为实验流程及结果展示与分析：

（1）实验准备

①数据集构建：构建包含不同模型通用能力维度（如语言理解与生成、知识能力、推理能力）和不同难度（简单、中等、困难）的问答数据集共509题，可确保统计分析的可靠性。同时每道题目包含问题描述、若干候选回答（由不同模型或人工生成），以及参考答案。制定明确的评分细则，涵盖多个评估维度，例如跨语言理解的翻译题得分点为：

5分：翻译准确，表达清晰流畅，完整传达原句的意思。且达到了信达雅的高度。

2分：翻译准确，表达清晰流畅，完整传达原句的意思。没有达到信达雅的高度，只是普通的翻译。

0分：翻译与原句意思相去甚远，或者语言表达错误连篇，难以理解，丧失了原句的含义。

②人工判分： 招募多名评委进行人工判分，每道题目的所有候选回答由两位评委独立评分。评委根据评分细则，在每个维度为每个回答打分。对每道题目的每个回答，取所有评委评分的平均值，作为人工判分的基准。

（2）模型评分

选择此次研究所用到的通用大模型（gpt4o-mini、DeepSeek-R1、abab6.5s-chat等）作为基础模型，定义不同组合策略，本次裁判实验选择的组合策略包括单个模型、两个模型组合、三个模型使用不同权重、五个最高分模型组合等。让每个大模型在数据集上基于评分标准生成评分回复及分数，并记录保存在csv文件中。将多个大模型判分后的分数作为基础，按照不同组合重新计算分数，得到单个模型的分数，两个模型的平均分，一模性与两个模型相组合后的平均分，五个模型的平均分等。

（3）计算相关性

①数据准备： 对于每道题目，整理人工判分的平均分和模型评分。并计算模型评分与人工判分的相关性，共使用两个指标，分别为斯皮尔曼相关系数（spearman）和皮尔逊相关系数（pearson）。得到的分数均保留2位小数。

②斯皮尔曼相关系数：计算模型评分与人工判分的排名相关性，适合非线性关系或非正态分布数据，使用公式：
$$
a=1 - \frac{6 \sum d_i^2}{n(n^2 - 1)}
$$
其中$d_i$是模型评分与人工评分的排名差，n是题目数量。

③皮尔逊相关系数：计算模型评分与人工评分的线性相关性，适合线性关系且数据近似正态分布，使用公式：
$$
r= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}
$$
其中$x_i$是模型评分，$y_i$是人工评分，$\bar{x}$、$\bar{y}$ 是均值。

④当皮尔逊与斯皮尔曼值均最高于0.8且接近时表明排名一致性和线性相关性都很强，且适合大多数场景，尤其是需要综合表现的任务，是此次试验需要的模型组合。

（4）结果及分析： 

①对两位评委之间的相关性进行分析，得到其斯皮尔曼和皮尔逊相关系数结果，其中pearson为0.77，spearman为0.74。

表明评委间的评分具有较强的相关性（通常>0.7为强相关），说明两位评委的评分标准较为一致，人工判分的平均分可以作为可靠的基准。

②根据斯皮尔曼和皮尔逊相关系数，排名所有模型组合。

优先选择在两种相关性指标上均表现优异的组合，优先考虑皮尔逊相关系高的结果排在前面，当两个模型组合该分数相同时再进行斯皮尔曼分数的比较。

此次实验选择多个通用模型作为基座模型，包括gpt4o-mini、DeepSeek-R1、abab6.5s-chat、glm4-9B、hunyuan-large、Moonshoot-v1-32k等，由于单个模型分数几乎均小于组合模型得分，因此在下表中忽略。其余组合方式及分数较好的gpt4o得到如下表结果，其中5个平均分为5个和gpt4o-mini组合取平均分后的得分，5个单模型分别为gpt4o-mini、abab6.5s-chat、hunyuan-large、Moonshoot-v1-32k、DeepSeek-R1：

表3.4裁判模型部分实验结果

| 模型名称                 | pearson | spearman |
| ------------------------ | ------- | -------- |
| 5 个平均分取平均分       | 0.84    | 0.82     |
| 5 个单模型最高分取平均分 | 0.83    | 0.80     |
| gpt4o+deepseek-r1&glm4   | 0.83    | 0.80     |
| gpt4o&abab6.5s           | 0.82    | 0.80     |
| gpt4o&hunyuan            | 0.82    | 0.80     |
| gpt4o&Moonshoot          | 0.82    | 0.79     |
| gpt4o&Deepseek-r1        | 0.81    | 0.78     |
| deepseek-r1&glm4         | 0.81    | 0.78     |
| gpt4o&deepseek-r1+glm4   | 0.80    | 0.77     |
| gpt4o&glm4               | 0.80    | 0.76     |
| gpt4o                    | 0.79    | 0.77     |

 

可以看出当使用5 个平均分取平均分，即gpt4o和abab6.5s的平均分、gpt4o和hunyuan的平均分、gpt4o&Moonshoot的平均分、gpt4o和Deepseek-r1的平均分、gpt4o和glm4的平均分加在一起取平均分时分数与人工评分的相关性最强，但需要的模型较多，可能不利于实际应用。

（5）结论

选择gpt4o-mini&abab6.5s-chat取平均分组合。理由如下：

①高相关性:据表3.4所示，两者值均>0.8，表明该组合在排名一致性和线性相关性上均表现良好，符合“两种相关性指标上均表现优异”的要求。

②评委间的相关性略低于该组合，说明该组合的评分与人工评分的对齐程度甚至高于评委间的一致性，体现了其可靠性，表示大模型组合可以满足评分需求。

③稳定性：两者值接近，该组合在不同评估维度（线性关系和排名一致性）上的表现均衡，稳定性较高。

④实际应用中的权衡:

顶级组合“5个平均分取平均分”需要过多模型，计算成本和复杂度更高，但在改进评分准确性上并无较大提升。

该组合仅用两个模型，相关性仅略低于顶级组合，且两个模型均为市面上调用较为便捷且性价比高的模型，在满足需求的情况下为最佳组合，便于进行后续实验。

[1] 